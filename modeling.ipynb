{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52009b7553754bbb9d51a35aa46233b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29341, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41411</td>\n",
       "      <td>I watched this film because I'm a big fan of R...</td>\n",
       "      <td>0</td>\n",
       "      <td>watch film m big fan river phoenix joaquin pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37586</td>\n",
       "      <td>It does not seem that this movie managed to pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>movie manage lot people see place bump acciden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6017</td>\n",
       "      <td>Enough is not a bad movie , just mediocre .</td>\n",
       "      <td>0</td>\n",
       "      <td>bad movie mediocre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44656</td>\n",
       "      <td>my friend and i rented this one a few nights a...</td>\n",
       "      <td>0</td>\n",
       "      <td>friend rent night ago single good movie see me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38711</td>\n",
       "      <td>Just about everything in this movie is wrong, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>movie wrong wrong wrong mike myers example s r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  sentiment  \\\n",
       "0  41411  I watched this film because I'm a big fan of R...          0   \n",
       "1  37586  It does not seem that this movie managed to pl...          1   \n",
       "2   6017        Enough is not a bad movie , just mediocre .          0   \n",
       "3  44656  my friend and i rented this one a few nights a...          0   \n",
       "4  38711  Just about everything in this movie is wrong, ...          0   \n",
       "\n",
       "                                    processed_review  \n",
       "0  watch film m big fan river phoenix joaquin pho...  \n",
       "1  movie manage lot people see place bump acciden...  \n",
       "2                                 bad movie mediocre  \n",
       "3  friend rent night ago single good movie see me...  \n",
       "4  movie wrong wrong wrong mike myers example s r...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data=pd.read_csv('./data/processed_train.csv')\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29341, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22622</td>\n",
       "      <td>Robert Lansing plays a scientist experimenting...</td>\n",
       "      <td>robert lansing play scientist experiment pass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10162</td>\n",
       "      <td>Well I've enjoy this movie, even though someti...</td>\n",
       "      <td>ve enjoy movie turn stereotypical situation nt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17468</td>\n",
       "      <td>First things first - though I believe Joel Sch...</td>\n",
       "      <td>thing believe joel schumacher well mediocre di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42579</td>\n",
       "      <td>I watched this movie on the grounds that Amber...</td>\n",
       "      <td>watch movie ground amber benson rock nick stah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>A certain sexiness underlines even the dullest...</td>\n",
       "      <td>certain sexiness underline dull tangent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                             review  \\\n",
       "0  22622  Robert Lansing plays a scientist experimenting...   \n",
       "1  10162  Well I've enjoy this movie, even though someti...   \n",
       "2  17468  First things first - though I believe Joel Sch...   \n",
       "3  42579  I watched this movie on the grounds that Amber...   \n",
       "4    701  A certain sexiness underlines even the dullest...   \n",
       "\n",
       "                                    processed_review  \n",
       "0  robert lansing play scientist experiment pass ...  \n",
       "1  ve enjoy movie turn stereotypical situation nt...  \n",
       "2  thing believe joel schumacher well mediocre di...  \n",
       "3  watch movie ground amber benson rock nick stah...  \n",
       "4            certain sexiness underline dull tangent  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_test_data=pd.read_csv('./data/processed_test.csv')\n",
    "print(imdb_test_data.shape)\n",
    "imdb_test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.62 ms, total: 1.62 ms\n",
      "Wall time: 1.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Concatenate train data and test data\n",
    "all_review = np.concatenate([imdb_data.processed_review.values, imdb_test_data.processed_review.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose MAX_LENGTH for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "\n",
    "for txt in all_review:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-471c6701d150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimdb_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'processed_review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Preprocess sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# Add `[CLS]` and `[SEP]`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# Max length to truncate/pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpad_to_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# Pad sentence to max length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# Return PyTorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_sent = tokenizer.encode_plus(\n",
    "    text=imdb_data.loc[7,'processed_review'],  # Preprocess sentence\n",
    "    add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "    max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "    pad_to_max_length=True,         # Pad sentence to max length\n",
    "    return_tensors='pt',           # Return PyTorch tensor\n",
    "    return_attention_mask=True      # Return attention mask\n",
    "    )\n",
    "encoded_sent.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=data,  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58682"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inept big screen remake avenger wild wild west'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_review[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['answer', 'person', 's', 'question', 'doesn', 'remember', 'film', 's', 'people', 'see', '1978', 's', 'show', 'tv', 's', 'video', 'd', 'news', 'era', 'mind', '##less', 'comedy', '70', 'movie', 'go', '##er', 'smart', 'avoid', 'film', 'love', 'billy', 'cry', '##tal', 'paul', 'l', '##yn', '##de', 'joan', 'river', 'paul', 'l', '##yn', '##de', 'funny', 'be', '##wi', '##tch', 'hollywood', 'square', 'joan', 'river', 'time', 'career', 'get', 'laugh', 'make', 'cruel', 'joke', 'singer', 'karen', 'carpenter', 's', 'lack', 'weight', 'ha', '##r', 'ha', '##r', 'joan', 'like', 'somewhat', 'famous', 'era', 'cast', 'surprising', 'doris', 'robert', 'later', 'everybody', 'love', 'raymond', 'somewhat', 'good', 'idea', 'storyline', 'man', 'get', 'pregnant', 'instead', 'woman', 'go', 'waste', 'help', 'male', 'friend', 'crystal', 'get', 'set', 'hooker', 'finally', 'lose', 'virginity', 'instead', 'get', 'pregnant', 'commentary', 'woman', 'take', 'position', 'power', 'away', 'man', 'crystal', 's', 'stomach', 'grow', 'go', 'female', 'emotion', 'relate', 'feeling', 'unfortunately', 'socially', 'misunderstood', 'out', '##cast', 's', 'attack', 'mob', 'wants', 'rub', 'guess', 's', 'force', 'sec', '##lusion', 'baby', 'barn', 'man', '##ger', 'god', 'knows', 'exit', 'e', '##w', '##w', '##w', '##w', 'turn', 'shock', 'girl', 'movie', 'worthless', 'forget', '##table', 'humor', 'high', 'school', 'level', '2', 'star', 'good', 'idea', 'good', 'touch', 'relevant', 'moment', 'w', 'billy', 'crystal', 'ignore', 'rest', 'ra', '##bit', '##t', 'test', 'flu', '##nk', 'big', 'time', 'believe', 'rod', '##dy', 'mcdowell', 'sign', 'end']\n",
      "[3437, 2711, 1055, 3160, 2987, 3342, 2143, 1055, 2111, 2156, 3301, 1055, 2265, 2694, 1055, 2678, 1040, 2739, 3690, 2568, 3238, 4038, 3963, 3185, 2175, 2121, 6047, 4468, 2143, 2293, 5006, 5390, 9080, 2703, 1048, 6038, 3207, 7437, 2314, 2703, 1048, 6038, 3207, 6057, 2022, 9148, 10649, 5365, 2675, 7437, 2314, 2051, 2476, 2131, 4756, 2191, 10311, 8257, 3220, 8129, 10533, 1055, 3768, 3635, 5292, 2099, 5292, 2099, 7437, 2066, 5399, 3297, 3690, 3459, 11341, 15467, 2728, 2101, 7955, 2293, 7638, 5399, 2204, 2801, 9994, 2158, 2131, 6875, 2612, 2450, 2175, 5949, 2393, 3287, 2767, 6121, 2131, 2275, 17074, 2633, 4558, 26970, 2612, 2131, 6875, 8570, 2450, 2202, 2597, 2373, 2185, 2158, 6121, 1055, 4308, 4982, 2175, 2931, 7603, 14396, 3110, 6854, 14286, 28947, 2041, 10526, 1055, 2886, 11240, 4122, 14548, 3984, 1055, 2486, 10819, 24117, 3336, 8659, 2158, 4590, 2643, 4282, 6164, 1041, 2860, 2860, 2860, 2860, 2735, 5213, 2611, 3185, 22692, 5293, 10880, 8562, 2152, 2082, 2504, 1016, 2732, 2204, 2801, 2204, 3543, 7882, 2617, 1059, 5006, 6121, 8568, 2717, 10958, 16313, 2102, 3231, 19857, 8950, 2502, 2051, 2903, 8473, 5149, 25005, 3696, 2203]\n"
     ]
    }
   ],
   "source": [
    "# tokenize sample\n",
    "tokens = tokenizer.tokenize(all_review[18])\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                 ``convert_tokens_to_ids`` method).\n\u001b[1;32m   2091\u001b[0m         \"\"\"\n\u001b[0;32m-> 2092\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2093\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2406\u001b[0m         )\n\u001b[1;32m   2407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2408\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2409\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m             )\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    429\u001b[0m                     )\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    432\u001b[0m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input nan is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_review = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_review]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_review])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
